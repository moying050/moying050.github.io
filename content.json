{"meta":{"title":"wei的博客","subtitle":"点点滴滴","description":null,"author":"Wei GAO","url":"https://moying050.github.io"},"pages":[{"title":"","date":"2017-03-12T09:13:00.427Z","updated":"2017-03-12T09:13:00.423Z","comments":false,"path":"tags/index.html","permalink":"https://moying050.github.io/tags/index.html","excerpt":"","text":""},{"title":"","date":"2017-03-12T09:13:42.111Z","updated":"2017-03-12T09:13:42.111Z","comments":false,"path":"categories/index.html","permalink":"https://moying050.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"并发控制相关","slug":"并发控制相关","date":"2017-03-13T14:52:01.000Z","updated":"2017-03-13T14:57:11.124Z","comments":true,"path":"2017/03/13/并发控制相关/","link":"","permalink":"https://moying050.github.io/2017/03/13/并发控制相关/","excerpt":"","text":"并发控制相关对锁的理解 资源的并发保护 保证临界区的顺序（内存屏障/CPU屏障） 当锁持有时间占比在90%以上时，不如让单线程去访问这个资源 并发控制的方法 互斥锁 读写锁 CAS 原子量 RCU 多版本控制 行锁 读写锁读写锁适合资源写少读多（多线程并发读比例高）的场景，而且linux的读写锁实现效率不高（单个写线程，20个读线程的情况下，如果没有很重的读开销，则使用读写锁反而比使用mutex要慢，当然如果读操作很耗时，则使用读写锁会更划算）。 linux下的pthread mutex实现，如果没有碰到竞争是不会陷入内核的。 RCURCU在内核中应用比较多，实现了读写的并行，在Writer端增加一定负担，使得Reader端几乎可以Zero-overhead，特别适用用读操作远远大与写操作的场景，或者写的优先级不如读高的场景。 MVCCMVCC让读不影响写，但读会读到旧数据。 场景决定应用，我们需要根据资源的读写比例来选择合适的并发控制方法。 CAS对无锁的理解 基本上一两次自旋尝试就能成功的情况下使用CAS效率比锁的效率高 使用一般锁会线程切换，如果极短时间就能拿锁，则线程切换的成本太高，反之亦然 通过减少阻塞和等待，来改进并发性和可扩展性 锁与锁之间的任务比较简单的时候，无锁编程更可行 CAS会增加竞争性，这点会影响性能，因此需要测试来确认 当临界区较短或者一个core上只有一个线程时，和mutex相比，使用spin lock性能较佳，因为避免了上下文切换。 事务内存这种新技术扩展了CAS的应用范围 使用无锁编程后，编译器乱序和指令乱序都遵循原则：内存执行顺序的基本原则–绝不修改单程序的行为，在多核情况下，则需要注意到乱序的存在。 行锁行锁是解决一个大数据下的场景问题：总数据量很大，但同一时刻并发操作的数据量相对是很少的，这种情况下，通过行锁的机制可以解决这种场景的需求。 举个例子，数据量有10亿，但同时刻并发量只有10w，则相当于10亿个数据共用了10w把锁，如果是普通锁机制，则需要10亿把锁。 原子量 对原子量的 ‘读出-计算-写入’ 操作序列是原子的，在以上动作完成之前，任何其它处理器和线程均无法访问该原子量。 对原子量的 ‘读出-计算-写入’ 操作序列是原子的，在以上动作完成之前，任何其它处理器和线程均无法访问该原子量。 性能方面的代价可以看内存屏障相关的内容 在X86体系结构下，对于64位架构，只要同时满足以下两个条件，那么对该基础内置数据类型变量（int、bool、指针等）的普通读写都是原子的： 条件1:该变量按cache line对齐 条件2:该变量sizeof值不超过64 内存屏障等疑问TODO在x86平台写无锁代码，是不是只要保证编译不乱序就可以？cache一致性如何处理，具体如何判断，如何操作 内存模型软件内存模型C11与C++11编程语言是一种弱的软件内存模型。如果你正在用x86/64等强类型处理器家族，当在这些语言中使用底层的原子操作时并不会受到影响注3。我之前说过，只要是为了阻止编译器乱序，你必须要指定正确的内存执行顺序限制. 硬件内存模型强内存模型意味着每个机器指令都能隐式的包含Acquire与Release语义。结果是，当一个CPU核执行一系列的写操作时，其它的每一个CPU核看见的都是那些值都以它们被写入时的顺序在改变。 顺序一致性在C++11中,当在原子数据类型执行操作的时候，可以使用默认的顺序约束，也就是memory_order_seq_cst。如果这么做了，工具链会限制编译器乱序并发出CPU特定的指令来充当合适的memory barrier。在这种方式下，尽管是在弱内存模型的多核设备上,也会模拟出顺序一致性的内存模型。 cpu cache相关 程序的运行存在时间和空间上的局部性 比如C语言中应该尽量减少静态变量的引用:静态变量存储在全局数据段，在一个被反复调用的函数体内，引用该变量需要对缓存多次换入换出，而如果是分配在堆栈上的局部变量，函数每次调用CPU只要从缓存中就能找到它了，因为堆栈的重复利用率高。 循环体内的代码要尽量精简:代码是放在指令缓存里的，而指令缓存都是一级缓存，只有几K字节大小，如果对某段代码需要多次读取，而这段代码又跨越一个L1缓存大小，那么缓存优势将荡然无存。 CPU的流水线(pipeline)并发性 内存屏障 store barrier，在x86 上是”sfence”指令，强迫所有的、在屏障指令之前的 存储指令在屏障以前发生，并且让 store buffers 刷新到发布这个指令的 CPU cache。这将使程序状态对其他 CPU 可见，这样，如果需要它们可以对它做出响应。 load barrier: 加载屏障，在x86 上是”lfence”指令，强迫所有的、加载指令之后的指令在屏障之后发生，然后等待那个 CPU 的 load buffer 排空。 Full Barrier，在x86 上是”mfence”指令，在 CPU 上是加载和内存屏障的组合。 原子指令和软件锁原子指令，如x86里的 “lock …” 指令，是高效的 full barrier，它们锁住存储子系统来执行操作，有受保证的全序关系（total order），即使跨 CPU。软件锁通常使用内存屏障，或原子指令来达到可视性和保留程序顺序。 Java 存储模型在Java 存储模型里，volatile 字段在写入后插入一个内存屏障，在读取前插入加载屏障。类里面修饰为 final 的字段在它们被初始化后插入一个存储指令，以确这些字段在构造函数完成、有可用引用到这个对象时是可见的。 内存屏障对性能的影响内存屏障阻止了 CPU 执行很多隐藏内存延迟的技术，因此有它们有显著的性能开销，必须考虑。为了达到最大性能，最好对问题建模，这样处理器可以做工作单元，然后让所有必须的内存屏障在工作单元的边界上发生。采用这种方法允许处理器不受限制地优化工作单元。把必须的内存屏障分组是有益的，那样，在第一个之后的 buffer 刷新的开销会小点，因为没有工作需要进行重新填充它。 参考文档 名不符实的读写锁 Read-Copy Update，向无锁编程进发! liburcu，一个用户态的RCU实现 HBase – 并发控制机制解析 Herb Sutter谈论C++无锁编程 并发编程系列之一：锁的意义 从LONGADDER看更高效的无锁实现 7个示例科普CPU CACHE 内存屏障（Memory Barriers） 无锁HASHMAP的原理与实现 Intel Haswell的事务内存分析 深入探索并发编程系列(一)-锁不慢；锁竞争慢 深入探索并发编程系列(二)-总是使用轻量级锁 深入探索并发编程系列(五)-将内存乱序逮个正着 深入探索并发编程系列(六)-编译期间内存乱序 深入探索并发编程系列(七)-内存屏障:资源控制操作 深入探索并发编程系列(八)-Acquire与Release语义 深入探索并发编程系列(九)-弱/强内存模型 High Performance Dynamic Lock-Free Hash Tablesand List-Based Sets","categories":[{"name":"程序设计","slug":"程序设计","permalink":"https://moying050.github.io/categories/程序设计/"}],"tags":[{"name":"高并发","slug":"高并发","permalink":"https://moying050.github.io/tags/高并发/"},{"name":"分布式","slug":"分布式","permalink":"https://moying050.github.io/tags/分布式/"}]},{"title":"ssd近期趋势","slug":"ssd近期趋势","date":"2017-03-12T15:39:57.000Z","updated":"2017-03-12T15:42:51.785Z","comments":true,"path":"2017/03/12/ssd近期趋势/","link":"","permalink":"https://moying050.github.io/2017/03/12/ssd近期趋势/","excerpt":"","text":"SSD相关趋势intel ssd种类以及定位 从性能上从低到高依此为HDD-&gt;3d nand -&gt; optane -&gt; 非易失性ram 3D NAND ssd是TLC的ssd，从TCO看已经可以代替10000转的sas硬盘，容量也很大，目前最大是16TB optane ssd是高性能ssd，会在17年二季度发布，随机读写性能都很强，为目前S3700十倍以上（之前的P3600读性能很好，但写只有S3700的2倍） intel在18年还会发布非易失性ram，据他们说性能与DDR2差不多，但掉电不会丢数据 自己做OP来延长ssd的寿命 OP指的是自己划出一块空间，保留给ssd，让ssd用来做加速/坏块映射等 百度自己的测试数据是他们拿10%的空间做OP，在他们的应用场景延长了1倍的寿命，随机写性能也有30%左右的提升,在我们的场景中也验证了 这种方式不论PCIE/sata ssd都是支持的 这种方式对于我们感觉有一定意义，可以以大空间换取高寿命 寿命计算也可以通过高版本smartctl工具的新字段（nand数据写入量）来自己近似计算 PCIE ssd会很快代替sata ssd17年2季度后pcie ssd可以使用raid/热插拔，已经可以完全取代sata ssd。 后续intel新技术不会再用到sata ssd上，只搞pcie ssd，性价比上pcie ssd也比sata ssd更好 供应上，即使在路线图中sata ssd还存在，但可能也会缺货 目前pcie ssd已经支持热插拔（m.2接口的pcie ssd） 17年二季度发布的v5系列cpu自带raid控制器，支持pcie ssd组raid（VMD/VROC），激活费用另算，raid1在99美元左右，raid性能也比raid卡的优秀稳定，不过可能对linux内核版本有一定要求 ssd的新形态支持1U服务器插32块ssd intel会推出ruler ssd，1U服务器最多可以插入32块ssd，每块ssd32TB，则可以支持1PB 会有新的nvme驱动方式来降低cpu吞吐这么多ssd的压力","categories":[{"name":"系统组建","slug":"系统组建","permalink":"https://moying050.github.io/categories/系统组建/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://moying050.github.io/tags/云存储/"},{"name":"硬件评估","slug":"硬件评估","permalink":"https://moying050.github.io/tags/硬件评估/"}]},{"title":"note of ambry","slug":"note-of-ambry","date":"2017-03-12T15:21:03.000Z","updated":"2017-03-12T15:32:57.628Z","comments":true,"path":"2017/03/12/note-of-ambry/","link":"","permalink":"https://moying050.github.io/2017/03/12/note-of-ambry/","excerpt":"看到ambry的设计文档，粗略做了下笔记，还没看代码实现，部分理解可能有出入。 LinkedIn的分布式对象存储系统 Ambry介绍Ambry由三部分组成，分别是用来存储和检索数据的一组数据节点，路由请求的前端机器（请求会在一些预处理之后路由到数据节点上）以及协调和维护集群的集群管理器。数据节点会在不同节点之间复制数据，同时支持数据中心内部和数据中间之间的复制。前端提供了支持对象PUT、GET和DELETE操作的HTTP API。另外，前端所使用的路由 库也可以直接用在客户端中，从而实现更好的性能。在LinkedIn，这些前端节点会作为CDN的原始服务器。ambry官方文档","text":"看到ambry的设计文档，粗略做了下笔记，还没看代码实现，部分理解可能有出入。 LinkedIn的分布式对象存储系统 Ambry介绍Ambry由三部分组成，分别是用来存储和检索数据的一组数据节点，路由请求的前端机器（请求会在一些预处理之后路由到数据节点上）以及协调和维护集群的集群管理器。数据节点会在不同节点之间复制数据，同时支持数据中心内部和数据中间之间的复制。前端提供了支持对象PUT、GET和DELETE操作的HTTP API。另外，前端所使用的路由 库也可以直接用在客户端中，从而实现更好的性能。在LinkedIn，这些前端节点会作为CDN的原始服务器。ambry官方文档 概览图 Clustermap Clustermap控制拓扑结构、维护状态并帮助协调集群的操作.数据节点和前端服务器都能够访问clustermap，并且会始终使用它们当前的视图来做出决策，这些决策涉及到选择可用的机器、过滤副本以及识别对象的位置等 硬件布局：包含了机器的列表、每台机器上的磁盘以及每个磁盘的容量。布局还维护资源的状态（机器和磁盘）并指定主机名和端口，通过主机名和端口就能连接到数据节点 分区布局：包含了分区的列表、它们的位置信息以及状态。在Ambry中，分区有一个数字表示的ID，副本的列表可以跨数据中心。分区是固定大小的资源，集群间的数据重平衡都是在分区级别进行的 partiton layout partition ID | state | replica ————-|————|————————————————— partition_0 | read/write | DC1:node1:disk_0 DC1:node4:disk_1 DC2:node2:disk_0 partition_1 | readonly | DC2:node0:disk_0 DC1:node4:disk_3 DC2:node2:disk_2 存储 索引本身也是持久化的文件，会定时更新，更新之间的时间窗之内的时间可以通过读取日志来重建。检查点应该对应持久化的索引文件组。磁盘log引入可以让写入顺序化，减少磁盘碎片 疑问 节点有文件删除，则日志中应该有数据可以丢弃，如何释放这部分空间 这里包含很多优化，不知道是什么 持久化 磁盘上的每个副本均被建模为预先分配的log（preallocated log），通过fallocate创建大文件，然后在里面append写入，保证顺序写入。 所有新的消息都会按照顺序附加到log上，消息是由实际的对象块（chunk）和相关的元数据（系统和用户）所组成的。 这能够使写入操作实现很高的吞吐量，并且避免出现磁盘碎片。 Ambry会使用索引将对象id与log中的消息映射起来，索引本身是一组排序的文件片段，条目按照最新使用在前，最旧的条目在后的顺序，从而便于高效查找。 索引中的每个条目都维护了log中消息的偏移量、消息的属性以及一些内部使用的域。 索引中的每个片段会维护一个bloom filter，从而优化实际磁盘操作所耗费的时间； 索引只有最新几个载入内存，其他都是map到内存地址，需要时才会读 零拷贝 通过使用sendfile API，在进行读取时，字节从log转移到网络的过程中实现了零拷贝。 通过避免额外的系统调用，实现了更好的性能，在这个过程中，会确保字节不会读入到用户内存中，不必进行缓存池的管理 读加速通过CDN，因此这里不需要读缓存 恢复 因为系统和机器会出现宕机，磁盘上的数据也有可能会损坏，所以有必要实现恢复（recovery）的功能。 在启动的时候，存储层会从最后一个已知的检查点读取log，并重建索引。 恢复也有助于重建内存中的状态。Log是恢复的来源，并且会永久保存 复制 存储节点还需要维护分区中各副本的同步。 每个节点上都会有一个复制服务（replication service），它会负责保证本地存储中的副本与所有的远程副本是同步的。 在这里，进行了很多的优化，以保证复制过程的高效可靠 step1 step2 step3 replica 2之前从其他节点同步过数据，因此可能需要检查自己还缺哪些blobid step4 step5 step6 step7 删除 删除的数据在落日志后还在磁盘上，空间没有释放 周期性扫描待删除的数据，跳过最近被删除的数据 删除数据会重写磁盘（至少把log中元数据相关清理掉），有模块会做流控防止io被删除占满 路由/前端 多主策略是如何实现的，如何仲裁 请求管理 路由会处理PUT、GET以及DELETE请求。 对于其中的每个请求类型，路由都会跟踪副本成功和失败的数量从而确定Quorum的值、维护分块的状态、生成对象id并在成功或失败的时候触发对应的回调 分块 对象会分解为块（chunk），每个块都能够跨分区独立地进行路由。 每个块都会有一个id来进行唯一标识。 路由会生成一个元数据对象，其中包含了块的列表以及它们所需的获取顺序。元数据对象存储为独立的blob，它的id也会作为blob的id。 在读取的时候，会得到元数据对象，然后检索各个块并返回给客户端 故障检测 故障检测的逻辑要负责主动识别宕机或状态出问题的资源。 资源可以是机器、磁盘或分区。 路由会将出现问题的资源标记为不可用，这样后续的请求就不会使用它们了 Quorum Ambry为写入和读取实现了一种多主人（multi-master）的策略。 这能够实现更高的可用性并减少端到端的延迟 这是通过减少一个额外的hop来实现的， 在基于主从结构（master slave）的系统中，往往会有这个额外的hop。请求通常会发往M个副本，然后等待至少N个成功的响应（这里N&lt;=M）。 路由会优先使用本地数据中心的副本，向其发送请求，如果本地存储无法实现所需的Quorum的话，它会代理远程数据中心的访问 变更捕获 在每次成功的PUT或DELETE操作之后，路由会生成一个变更捕获（change capture）。 变更捕获中所包含的信息是blob id以及blob相关的元数据，这个信息可以被下游的应用所使用。 文件的写入删除会可以通知下游应用，比如kafka 安全控制需求 多租户引入需要安全保证 使用SSL/TLS做传输的加密 对于哪些传输需要加密可配置，默认不加密 框架 数据传递需要考虑数据传输可以支持异步读写，内存拷贝减少，压力反馈。在层与层之间传递数据通过传递相关类实现，避免了过多的内存拷贝 Scaling Layer 中间层执行非阻塞操作，传递数据 RestRequestHandler 处理由NIO layer发起的请求 内部由多个单元（类似多线程），单元多少影响数据吞吐量和延迟 RestResponseHandler 处理由Remote Service Layer发起的response 内部由多个单元（类似多线程），单元多少影响数据吞吐量和延迟 AsyncRequestResponseHandler 内部包含了RestRequestHandler和RestResponseHandler 处理请求和回复是异步的 请求由一个或多个单元处理，称为 AsyncRequestWorker AsyncRequestWorker需要维护状态：等待处理的请求/正在处理的请求/等待被发送出去，回复的请求 Remote Service Layer： 底层网络相关处理以及路由处理 这一层是单例无状态的，数据状态都存放于NIO layer初始化，用于数据传输的ReadableStreamChannel/RestResponseChannel上 NIO layer: 上层对外的网络服务 负责对外所有网络相关操作 接收端：比如HTTP编解码/接收来自client的请求，处理/向下传递 发送端：将回复发送给client 维护一些状态： 维护能用来处理请求的RestRequestHandler 每个channel也要维护其中请求的状态 正在处理的RestRequest RestResponseChannel对于每个请求也维护各自状态，方便处理对应的错误 部件交互收包NIO layer将包解出来，以RestRequest的形式搭配RestResponseChannel(用来传递回复消息)传递给RestRequestHandler，以做异步处理 包的异步处理线程从request queue中取出rest request，做不同处理 handleGet：AmbryBlobStorageService从request中解出blob id，做相应处理交互 GET request：获取blob属性和内容，之前先创建 Callback object，待前面获取到了之后回调 GET handlePost: ReadableStreamChannel在这里的作用是在失败时或成功时返回结果以及反馈线路上的压力 在路由侧，接收推送的数据时，读不是有数据就去读的，而是等前一波数据推完了再读下一批，不通过事件机制读取数据 原因是这样可以给发送数据一侧反馈压力（利用tcp本身的机制） POST handleDelete： DELETE handleHead： HEAD","categories":[{"name":"系统设计","slug":"系统设计","permalink":"https://moying050.github.io/categories/系统设计/"}],"tags":[{"name":"云存储","slug":"云存储","permalink":"https://moying050.github.io/tags/云存储/"},{"name":"分布式","slug":"分布式","permalink":"https://moying050.github.io/tags/分布式/"}]}]}